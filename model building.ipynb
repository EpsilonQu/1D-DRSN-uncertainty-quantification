{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b507a-0150-4a90-a428-b2f295683381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import (Activation, AveragePooling1D, BatchNormalization,\n",
    "                          Conv1D, Dense, GlobalAveragePooling1D, Input)\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Nadam\n",
    "from keras.regularizers import l2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import generic_utils\n",
    "from keras.utils import np_utils,plot_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad9fab-254a-4b7e-a9c1-c0cc34164acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn(\n",
    "      untransformed_scale_initializer=tf.compat.v1.initializers.random_normal(\n",
    "          mean=0,\n",
    "          stddev=1),)\n",
    "kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (x_train.shape[0] *1.0)\n",
    "def abs_backend(inputs):\n",
    "    return K.abs(inputs)\n",
    "\n",
    "def expand_dim_backend(inputs):\n",
    "    return K.expand_dims(inputs,1)\n",
    "\n",
    "def sign_backend(inputs):\n",
    "    return K.sign(inputs)\n",
    "\n",
    "def pad_backend(inputs, in_channels, out_channels):\n",
    "    pad_dim = (out_channels - in_channels)//2\n",
    "    inputs = K.expand_dims(inputs)\n",
    "    inputs = K.spatial_2d_padding(inputs,padding=((0,0),(pad_dim,pad_dim)))\n",
    "    return K.squeeze(inputs,-1)\n",
    "\n",
    "def residual_shrinkage_block(incoming, nb_blocks, out_channels, downsample=False,\n",
    "                             downsample_strides=2):\n",
    "    residual = incoming\n",
    "    in_channels = incoming.get_shape().as_list()[-1]\n",
    "    \n",
    "    for _ in range(nb_blocks):\n",
    "        \n",
    "        identity = residual\n",
    "        \n",
    "        if not downsample:\n",
    "            downsample_strides = 1\n",
    "        \n",
    "        residual = BatchNormalization()(residual)\n",
    "        residual = Activation('relu')(residual)\n",
    "        residual = Conv1D(out_channels, 3, strides=downsample_strides, \n",
    "                          padding='same', kernel_initializer='he_normal', \n",
    "                          kernel_regularizer=l2(1e-4))(residual)\n",
    "        \n",
    "        residual = BatchNormalization()(residual)\n",
    "        residual = Activation('relu')(residual)\n",
    "        residual = Conv1D(out_channels, 3, padding='same', kernel_initializer='he_normal', \n",
    "                          kernel_regularizer=l2(1e-4))(residual)\n",
    "        \n",
    "        residual_abs = Lambda(abs_backend)(residual)\n",
    "        abs_mean = GlobalAveragePooling1D()(residual_abs)\n",
    "        scales = Dense(out_channels, activation=None, kernel_initializer='he_normal', \n",
    "                       kernel_regularizer=l2(1e-4))(abs_mean)\n",
    "        scales = BatchNormalization()(scales)\n",
    "        scales = Activation('relu')(scales)\n",
    "        scales = Dense(out_channels, activation='sigmoid', kernel_regularizer=l2(1e-4))(scales)\n",
    "        scales = Lambda(expand_dim_backend)(scales)\n",
    "        \n",
    "        thres = keras.layers.multiply([abs_mean, scales])\n",
    "        \n",
    "        sub = keras.layers.subtract([residual_abs, thres])\n",
    "        zeros = keras.layers.subtract([sub, sub])\n",
    "        n_sub = keras.layers.maximum([sub, zeros])\n",
    "        residual = keras.layers.multiply([Lambda(sign_backend)(residual), n_sub])\n",
    "        \n",
    "        if downsample_strides > 1:\n",
    "            identity = AveragePooling1D(pool_size=1, strides=2)(identity)\n",
    "            \n",
    "        if in_channels != out_channels:\n",
    "\n",
    "            identity = Lambda(pad_backend, arguments={'in_channels':in_channels,'out_channels':out_channels})(identity)\n",
    "\n",
    "\n",
    "\n",
    "        residual = keras.layers.add([residual, identity])\n",
    "    \n",
    "    return residual\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    inputs = 16\n",
    "    outputs = 2\n",
    "\n",
    "    x_input = Input(shape=(inputs,1))\n",
    "    x = Conv1D(4,3,strides=2,padding='same')(x_input)\n",
    "    x = Conv1D(4,3,2,padding='same')(x)\n",
    "    x = residual_shrinkage_block(x, 1, 4, downsample=False)\n",
    "    x = residual_shrinkage_block(x, 3, 4, downsample=False)\n",
    "\n",
    "    x = residual_shrinkage_block(x, 1, 8, downsample=True)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = Dense(outputs,activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=x_input,outputs=x)\n",
    "    optimizers = Nadam(lr=1e-5)\n",
    "    model.compile(optimizer = optimizers, loss= 'categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    model.fit(x_train, y_train, validation_data=(x_test,y_test),epochs=100, batch_size=512, verbose=1)\n",
    "    pred_vi=np.zeros((len(x_val),4))\n",
    "    pred_max_p_vi=np.zeros((len(x_val)))\n",
    "    pred_std_vi=np.zeros((len(x_val)))\n",
    "    entropyaleatoric_vi = np.zeros((len(x_val)))\n",
    "    entropyepistemic_vi = np.zeros((len(x_val)))\n",
    "    for i in tqdm(range(0,len(x_val))):\n",
    "\n",
    "        multi_img=np.tile(x_val[i],(5,1,1))\n",
    "        preds=model.predict(multi_img)\n",
    "        pred_vi[i]=np.mean(preds,axis=0)#mean over n runs of every proba class\n",
    "        for j in range(0,4):\n",
    "            for z in range (0,5):\n",
    "                entropyaleatoric_vi[i] = -((np.sum(preds[z][j] * np.log2(preds[z][j] + 1E-14)))/5) #Numerical Stability\n",
    "                entropyepistemic_vi[i] = (-np.sum(pred_vi[i][j] * np.log2(pred_vi[i][j] + 1E-14)))+((np.sum(preds[z][j] * np.log2(preds[z][j] + 1E-14)))/5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
